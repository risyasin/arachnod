{"name":"Arachnod","tagline":"High performance crawler for Nodejs","body":"Powerful & Easy to use web crawler for Nodejs.  [Arachnod](http://arachnod.evrima.net) has been designed for heavy and long running tasks, \r\nfor performance & efective resource usage. For it's goals Arachnod uses [Redis](http://www.redis.io)'s power as a backend. \r\nCovering all heavy & time consuming tasks such as controlling urls & their tasks to store & distribute information among the Arachnod's child tasks \r\n(Spiderlings). Arachnod also avoids to use any server-side DOM requiring technics \r\nsuch as [jQuery](http://www.jquery.com) with [JSdom](https://github.com/tmpvar/jsdom) to use resources properly. \r\nFrankly, I have tested JSdom for along time with no luck, always memory leaks & high memory usage. \r\nLibxml based XPath solutions were not actually real, Instead, Arachnod uses [Cheerio](http://cheeriojs.github.io/cheerio/) for accessing DOM elements. \r\nAlso uses [SuperAgent](https://github.com/visionmedia/superagent) as HTTP Client. \r\n\r\n\r\n### How to install \r\n\r\n`$ npm install arachnod`\r\n    \r\nOr via Git\r\n`$ git clone git@github.com:risyasin/arachnod.git`\r\n    \r\nThen, install required Nodejs modules with npm \r\n`$ npm install`\r\n    \r\n**Please** make sure you have a running [redis-server](https://redis.io) \r\n\r\n### How to use\r\n\r\n``` js\r\nvar bot = require('arachnod');\r\n    \r\nbot.on('hit', function (doc, $) {\r\n    \r\n    // Do whatever you want to do parsed html content.\r\n    \r\n    var desc = $('article.entry-content').text();\r\n    \r\n    console.log(doc.path, desc);\r\n    \r\n    // if you don't need to follow all links.\r\n    bot.pause();\r\n    \r\n    \r\n});\r\n\r\n\r\nbot.crawl({\r\n    'redis': '127.0.0.1',\r\n    'parallel': 4,\r\n    'start': 'https://github.com/risyasin/arachnod',\r\n    'resume': false\r\n});\r\n\r\n\r\nbot.on('error', function (err, task) {\r\n    log('Bot error:', err, err.stack, task);\r\n});\r\n\r\n\r\nbot.on('end', function (err, status) {\r\n    log('Bot finished:', err, status);\r\n});\r\n```\r\n\r\n\r\n### Documentation \r\n\r\n\r\n##### Parameters\r\nParameter Name  | Description\r\n------------- | -------------\r\n**start** | Start url for crawling (Mandatory) \r\n**parallel** | Number of child processes that will handle network tasks (Default: 8) Do not this more than 20. \r\n**redis** | Host name or IP address that Redis runs on (Default: 127.0.0.1)\r\n**redisPort**  | Port number for Redis (Default: 6379)\r\n**verbose**  | Arachnod will tell you more, **1** (silence) - **10** (everything). Default: 1.\r\n**resume**  | Resume support, Simply, does not resets queues if there is any. (Default: false) \r\n**ignorePaths**  | Ignores paths starts with. Must be multiple in array syntax such as `['/blog','/gallery']` \r\n**ignoreParams**  | Ignores query string parameters, Must be in array syntax. such as `['color','type']`  \r\n**sameDomain**  | Stays in the same hostname. (will be implemented at v1)\r\n**useCookies**  | Using cookies (will be implemented at v0.4)\r\n**obeyRobotsTxt**  | As it's name says. Honors the robots.txt (will be implemented at v0.5) \r\n \r\n\r\n\r\n\r\n##### Events\r\nEvent Name  | Description\r\n------------- | -------------\r\n**hit** | Emits when a url has been downloaded & processed, sends two parameters in order *doc* Parsed url info, **$** as Cheerio object. \r\n**error**  | Emits when an error occurs at any level including child processes. Single parameter Error or Exception.  \r\n**end**  | Emits when reached at the end of tasks queue. Return statistics.  \r\n**stats**  | Emits bot stats whenever a child changes it's states (such as downloading or querying queues). Use wisely.  \r\n\r\n\r\n\r\n\r\n##### Methods\r\nMethod Name  | Description\r\n------------- | -------------\r\n**crawl(Parameters)** | Starts a new crawling session with parameters\r\n**pause()**  | Stops bot but does not delete any task queue.\r\n**resume()**  | Starts back a paused session. Useful to control resource usage in low spec systems (single core etc.). \r\n**queue(url)**  | Adds given url to task queue. \r\n**getStats()**  | Returns various statistics such as downloaded, checked, finished url counts, memory size etc. \r\n \r\n\r\n \r\n \r\n##### What's Next\r\n* Regex support for ignore parameters\r\n* Cookie support\r\n* Robots.txt & rel=nofollow support\r\n* Actions for content-type or any given response headers \r\n* Custom headers\r\n* Custom POST/PUT method queues\r\n* Free-Ride mode (will be fun)\r\n* Stats for each download/hit event\r\n* Plugin support\r\n\r\n\r\n\r\n#### Support \r\nIf you love to use Arachnod. Help me to improve it. \r\nFeel free to make pull request for anything useful. \r\n\r\n\r\n#### License\r\n\r\nGNU - Version 2\r\n","google":"UA-25703095-2","note":"Don't delete this file! It's used internally to help with page regeneration."}